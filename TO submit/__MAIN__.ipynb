{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wildlife-datasets\n",
      "  Downloading wildlife_datasets-1.0.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (2.1.4)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (4.65.0)\n",
      "Collecting opencv-python>=4.5.5.62 (from wildlife-datasets)\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (10.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (1.4.1.post1)\n",
      "Requirement already satisfied: matplotlib>=3.5.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (3.8.3)\n",
      "Collecting gdown (from wildlife-datasets)\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting kaggle (from wildlife-datasets)\n",
      "  Downloading kaggle-1.6.12.tar.gz (79 kB)\n",
      "     ---------------------------------------- 0.0/79.7 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/79.7 kB ? eta -:--:--\n",
      "     ----------------------------- -------- 61.4/79.7 kB 825.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 79.7/79.7 kB 746.4 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda_jupyter2024\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from pandas>=1.1.4->wildlife-datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from pandas>=1.1.4->wildlife-datasets) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets) (3.4.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda_jupyter2024\\lib\\site-packages (from tqdm>=4.62.3->wildlife-datasets) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from gdown->wildlife-datasets) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from gdown->wildlife-datasets) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in d:\\anaconda_jupyter2024\\lib\\site-packages (from gdown->wildlife-datasets) (2.31.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from kaggle->wildlife-datasets) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-slugify in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (2.0.7)\n",
      "Requirement already satisfied: bleach in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (4.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from beautifulsoup4->gdown->wildlife-datasets) (2.5)\n",
      "Requirement already satisfied: webencodings in d:\\anaconda_jupyter2024\\lib\\site-packages (from bleach->kaggle->wildlife-datasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from python-slugify->kaggle->wildlife-datasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets) (1.7.1)\n",
      "Downloading wildlife_datasets-1.0.3-py3-none-any.whl (43 kB)\n",
      "   ---------------------------------------- 0.0/43.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 43.3/43.3 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/38.6 MB 2.2 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 0.3/38.6 MB 3.9 MB/s eta 0:00:10\n",
      "    --------------------------------------- 0.7/38.6 MB 5.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.2/38.6 MB 6.6 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.8/38.6 MB 7.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.3/38.6 MB 8.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.3/38.6 MB 8.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.3/38.6 MB 8.1 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.3/38.6 MB 8.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.5/38.6 MB 7.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.8/38.6 MB 9.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.4/38.6 MB 9.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 5.9/38.6 MB 9.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.4/38.6 MB 10.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 6.9/38.6 MB 10.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 7.5/38.6 MB 10.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 8.0/38.6 MB 10.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 8.5/38.6 MB 10.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.0/38.6 MB 10.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.5/38.6 MB 10.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 10.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.6/38.6 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.1/38.6 MB 11.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 11.6/38.6 MB 11.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.1/38.6 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 12.2/38.6 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 13.2/38.6 MB 12.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 13.7/38.6 MB 12.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 14.3/38.6 MB 11.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 14.8/38.6 MB 11.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 15.3/38.6 MB 11.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.9/38.6 MB 11.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 16.4/38.6 MB 11.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 11.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 17.5/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 18.1/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 18.6/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 19.2/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.7/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.3/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.8/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 21.2/38.6 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 21.9/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.4/38.6 MB 12.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 23.0/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.5/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 24.1/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.6/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.2/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.7/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.3/38.6 MB 11.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 26.8/38.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.5/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.6/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.1/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.7/38.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.2/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.8/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.6 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.6 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.2/38.6 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 31.9/38.6 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.5/38.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.0/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.5/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.0/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.5/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.0/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.5/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.1/38.6 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.1/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 4.7 MB/s eta 0:00:00\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.12-py3-none-any.whl size=102982 sha256=eee2b1869a103fef1a71941e127e003cb248b683bc8d653496a3a960c8416f33\n",
      "  Stored in directory: c:\\users\\moham\\appdata\\local\\pip\\cache\\wheels\\f3\\eb\\e9\\819c2d9eac90204eec8579430759f75a1d6dbe4cd0b93f53bc\n",
      "Successfully built kaggle\n",
      "Installing collected packages: opencv-python, kaggle, gdown, wildlife-datasets\n",
      "Successfully installed gdown-5.1.0 kaggle-1.6.12 opencv-python-4.9.0.80 wildlife-datasets-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wildlife-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET ATRW: DOWNLOADING STARTED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "atrw_detection_test.tar.gz: 0.00B [00:01, ?B/s]\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: The specified resource does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwildlife_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, splits\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Download dataset\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m datasets\u001b[38;5;241m.\u001b[39mATRW\u001b[38;5;241m.\u001b[39mget_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ATRW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Load metadata\u001b[39;00m\n\u001b[0;32m      8\u001b[0m metadata \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mATRW(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ATRW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_datasets\\datasets\\datasets.py:78\u001b[0m, in \u001b[0;36mDatasetFactory.get_data\u001b[1;34m(cls, root, force, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATASET \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: DOWNLOADING STARTED.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_name)\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdownload(root, force\u001b[38;5;241m=\u001b[39mforce, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATASET \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: EXTRACTING STARTED.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dataset_name)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mextract(root,  \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_datasets\\datasets\\datasets.py:96\u001b[0m, in \u001b[0;36mDatasetFactory.download\u001b[1;34m(cls, root, force, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     os\u001b[38;5;241m.\u001b[39mremove(mark_file_name)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mdata_directory(root):\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_download(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mopen\u001b[39m(mark_file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicenses_url\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata:\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_datasets\\datasets\\datasets.py:659\u001b[0m, in \u001b[0;36mATRW._download\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m url, archive \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdownloads:\n\u001b[1;32m--> 659\u001b[0m         utils\u001b[38;5;241m.\u001b[39mdownload_url(url, archive)\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Evaluation scripts\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     utils\u001b[38;5;241m.\u001b[39mdownload_url(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39murl, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39marchive)\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_datasets\\datasets\\utils.py:134\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, output_path)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_url\u001b[39m(url, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m, unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, miniters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39murl\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m t:\n\u001b[1;32m--> 134\u001b[0m         urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(url, filename\u001b[38;5;241m=\u001b[39moutput_path, reporthook\u001b[38;5;241m=\u001b[39mt\u001b[38;5;241m.\u001b[39mupdate_to)\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(urlopen(url, data)) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\ANACONDA_Jupyter2024\\Lib\\urllib\\request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: The specified resource does not exist."
     ]
    }
   ],
   "source": [
    "#Import wildlife-datasets Library\n",
    "from wildlife_datasets import datasets, splits\n",
    "\n",
    "#Download dataset\n",
    "datasets.ATRW.get_data('data/ATRW')\n",
    "\n",
    "#Load metadata\n",
    "metadata = datasets.ATRW('data/ATRW')\n",
    "\n",
    "#Get 80/20 training/test split\n",
    "splitter = splits.ClosedSetSplit(0.8)\n",
    "splitter.split(metadata.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import numpy as np\n",
    "from wildlife_datasets.datasets import MacaqueFaces\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, splits\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from wildlife_tools.features import SIFTFeatures\n",
    "\n",
    "\n",
    "# Download dataset (if not already downloaded)\n",
    "datasets.CowDataset.get_data('../data/CowDataset')\n",
    "\n",
    "# Load dataset metadata\n",
    "metadata_CZoo = datasets.CowDataset('../data/CowDataset')\n",
    "transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = WildlifeDataset(metadata_CZoo.df, metadata_CZoo.root, transform=transform)\n",
    "\n",
    "dataset_database_CZoo = WildlifeDataset(metadata_CZoo.df.iloc[100:,:], metadata_CZoo.root, transform=transform)\n",
    "dataset_query_CZoo = WildlifeDataset(metadata_CZoo.df.iloc[:100,:], metadata_CZoo.root, transform=transform)\n",
    "\n",
    "name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n",
    "extractor_CZoo = SIFTFeatures()\n",
    "query_CZoo = extractor_CZoo(dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wildlife-tools in d:\\anaconda_jupyter2024\\lib\\site-packages (0.0.9)\n",
      "Requirement already satisfied: torch>=2.0.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (2.2.2+cu121)\n",
      "Requirement already satisfied: timm>=0.9.2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (0.9.16)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-tools) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (2.1.4)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (4.65.0)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.62 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (4.9.0.80)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-tools) (10.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-tools) (1.4.1.post1)\n",
      "Requirement already satisfied: pycocotools in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (2.0.7)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-tools) (2.16.2)\n",
      "Requirement already satisfied: pytorch-metric-learning in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (2.5.0)\n",
      "Requirement already satisfied: transformers>=4.30.2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (4.40.2)\n",
      "Requirement already satisfied: wildlife-datasets>=0.3.4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (1.0.3)\n",
      "Requirement already satisfied: kornia>=0.6.12 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (0.7.2)\n",
      "Requirement already satisfied: faiss-cpu in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-tools) (1.8.0)\n",
      "Requirement already satisfied: kornia-rs>=0.1.0 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kornia>=0.6.12->wildlife-tools) (0.1.3)\n",
      "Requirement already satisfied: packaging in d:\\anaconda_jupyter2024\\lib\\site-packages (from kornia>=0.6.12->wildlife-tools) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.1.4->wildlife-tools) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from pandas>=1.1.4->wildlife-tools) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from pandas>=1.1.4->wildlife-tools) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-tools) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-tools) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-tools) (3.4.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from timm>=0.9.2->wildlife-tools) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm>=0.9.2->wildlife-tools) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm>=0.9.2->wildlife-tools) (0.23.0)\n",
      "Requirement already satisfied: safetensors in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm>=0.9.2->wildlife-tools) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.1->wildlife-tools) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.1->wildlife-tools) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.1->wildlife-tools) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.1->wildlife-tools) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.1->wildlife-tools) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.1->wildlife-tools) (2024.3.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda_jupyter2024\\lib\\site-packages (from tqdm>=4.62.3->wildlife-tools) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda_jupyter2024\\lib\\site-packages (from transformers>=4.30.2->wildlife-tools) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda_jupyter2024\\lib\\site-packages (from transformers>=4.30.2->wildlife-tools) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\anaconda_jupyter2024\\lib\\site-packages (from transformers>=4.30.2->wildlife-tools) (0.19.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets>=0.3.4->wildlife-tools) (3.8.3)\n",
      "Requirement already satisfied: gdown in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets>=0.3.4->wildlife-tools) (5.1.0)\n",
      "Requirement already satisfied: kaggle in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets>=0.3.4->wildlife-tools) (1.6.12)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (1.62.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\anaconda_jupyter2024\\lib\\site-packages (from tensorboard->wildlife-tools) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard->wildlife-tools) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife-tools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife-tools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife-tools) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife-tools) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets>=0.3.4->wildlife-tools) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from werkzeug>=1.0.1->tensorboard->wildlife-tools) (2.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from gdown->wildlife-datasets>=0.3.4->wildlife-tools) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets>=0.3.4->wildlife-tools) (2024.2.2)\n",
      "Requirement already satisfied: python-slugify in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets>=0.3.4->wildlife-tools) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets>=0.3.4->wildlife-tools) (2.0.7)\n",
      "Requirement already satisfied: bleach in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets>=0.3.4->wildlife-tools) (4.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests->transformers>=4.30.2->wildlife-tools) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests->transformers>=4.30.2->wildlife-tools) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch>=2.0.1->wildlife-tools) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from beautifulsoup4->gdown->wildlife-datasets>=0.3.4->wildlife-tools) (2.5)\n",
      "Requirement already satisfied: webencodings in d:\\anaconda_jupyter2024\\lib\\site-packages (from bleach->kaggle->wildlife-datasets>=0.3.4->wildlife-tools) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from python-slugify->kaggle->wildlife-datasets>=0.3.4->wildlife-tools) (1.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets>=0.3.4->wildlife-tools) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wildlife-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wildlife-datasets in d:\\anaconda_jupyter2024\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.19.4 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (2.1.4)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (4.65.0)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.62 in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (4.9.0.80)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (10.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (1.4.1.post1)\n",
      "Requirement already satisfied: matplotlib>=3.5.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from wildlife-datasets) (3.8.3)\n",
      "Requirement already satisfied: gdown in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (5.1.0)\n",
      "Requirement already satisfied: kaggle in d:\\anaconda_jupyter2024\\lib\\site-packages (from wildlife-datasets) (1.6.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda_jupyter2024\\lib\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.5.1->wildlife-datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from pandas>=1.1.4->wildlife-datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from pandas>=1.1.4->wildlife-datasets) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn>=1.0.1->wildlife-datasets) (3.4.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda_jupyter2024\\lib\\site-packages (from tqdm>=4.62.3->wildlife-datasets) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda_jupyter2024\\lib\\site-packages (from gdown->wildlife-datasets) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from gdown->wildlife-datasets) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in d:\\anaconda_jupyter2024\\lib\\site-packages (from gdown->wildlife-datasets) (2.31.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from kaggle->wildlife-datasets) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-slugify in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (2.0.7)\n",
      "Requirement already satisfied: bleach in d:\\anaconda_jupyter2024\\lib\\site-packages (from kaggle->wildlife-datasets) (4.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from beautifulsoup4->gdown->wildlife-datasets) (2.5)\n",
      "Requirement already satisfied: webencodings in d:\\anaconda_jupyter2024\\lib\\site-packages (from bleach->kaggle->wildlife-datasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in d:\\anaconda_jupyter2024\\lib\\site-packages (from python-slugify->kaggle->wildlife-datasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests[socks]->gdown->wildlife-datasets) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wildlife-datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in d:\\anaconda_jupyter2024\\lib\\site-packages (0.9.16)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm) (2.2.2+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from timm) (0.17.2)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm) (0.23.0)\n",
      "Requirement already satisfied: safetensors in d:\\anaconda_jupyter2024\\lib\\site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->timm) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->timm) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda_jupyter2024\\lib\\site-packages (from huggingface_hub->timm) (23.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda_jupyter2024\\lib\\site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from huggingface_hub->timm) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub->timm) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch->timm) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torch->timm) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from torchvision->timm) (10.3.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda_jupyter2024\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests->huggingface_hub->timm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda_jupyter2024\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\moham\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch->timm) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acrface_Loss accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import timm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import SGD\n",
    "\n",
    "from wildlife_tools.data import WildlifeDataset, SplitMetadata\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "\n",
    "import timm\n",
    "import numpy as np\n",
    "from wildlife_datasets.datasets import MacaqueFaces\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, splits\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset (if not already downloaded)\n",
    "datasets.IPanda50.get_data('../data/IPanda50')\n",
    "# Load dataset metadata\n",
    "metadata_P = datasets.IPanda50('../data/IPanda50')\n",
    "transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset_P = WildlifeDataset(metadata_P.df, metadata_P.root, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import timm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import SGD\n",
    "\n",
    "from wildlife_tools.data import WildlifeDataset, SplitMetadata\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "\n",
    "import timm\n",
    "import numpy as np\n",
    "from wildlife_datasets.datasets import MacaqueFaces\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, splits\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET IPanda50: DOWNLOADING STARTED.\n",
      "You are trying to download an already downloaded dataset.\n",
      "        This message may have happened to due interrupted download or extract.\n",
      "        To force the download use the `force=True` keyword such as\n",
      "        get_data(..., force=True) or download(..., force=True).\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████| 54/54 [34:14<00:00, 38.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 31.56949234008789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████| 54/54 [35:16<00:00, 39.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Loss = 25.097501754760742\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import itertools\n",
    "from torch.optim import SGD\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "\n",
    "# Download dataset (if not already downloaded)\n",
    "datasets.IPanda50.get_data('../data/IPanda50')\n",
    "# Load dataset metadata\n",
    "metadata = datasets.IPanda50('../data/IPanda50')\n",
    "transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n",
    "\n",
    "\n",
    "# Download MegaDescriptor-T backbone from HuggingFace Hub\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n",
    "\n",
    "# Arcface loss - needs backbone output size and number of classes.\n",
    "objective = ArcFaceLoss(\n",
    "    num_classes=dataset.num_classes,\n",
    "    embedding_size=768,\n",
    "    margin=0.5,\n",
    "    scale=64\n",
    "    )\n",
    "\n",
    "# Optimize parameters in backbone and in objective using single optimizer.\n",
    "params = itertools.chain(backbone.parameters(), objective.parameters())\n",
    "optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n",
    "\n",
    "def print_epoch_loss(trainer, epoch_data):\n",
    "    # This function will print the average loss at the end of each epoch\n",
    "    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n",
    "\n",
    "\n",
    "trainer = BasicTrainer(\n",
    "    dataset=dataset,\n",
    "    model=backbone,\n",
    "    objective=objective,\n",
    "    optimizer=optimizer,\n",
    "    epochs=2,\n",
    "    device='cuda',\n",
    "    epoch_callback=print_epoch_loss\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_retrained_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you need to use the model for further operations\n",
    "# # Initialize the model architecture again as needed for loading\n",
    "# # Make sure to initialize it exactly as you did for training\n",
    "# model = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=False)\n",
    "\n",
    "# # # Create a BasicTrainer instance with the initialized model (other parameters should match the training setup)\n",
    "# # trainer = BasicTrainer(\n",
    "# #     dataset=None,  # Assuming no dataset is needed just for loading\n",
    "# #     model=model,\n",
    "# #     objective=None,  # Assuming no objective needed just for loading\n",
    "# #     optimizer=None,  # Assuming no optimizer needed just for loading\n",
    "# #     epochs=0,  # No training epochs needed for just loading\n",
    "# #     device='cuda'  # Adjust as per your device configuration\n",
    "# # )\n",
    "\n",
    "# trainer = BasicTrainer(\n",
    "#     dataset=dataset,\n",
    "#     model=backbone,\n",
    "#     objective=objective,\n",
    "#     optimizer=optimizer,\n",
    "#     epochs=2,\n",
    "#     device='cuda',\n",
    "#     epoch_callback=print_epoch_loss\n",
    "# )\n",
    "# # Load the model\n",
    "# trainer.load(\"retrained_chks/arcfaceloass_retrained_checkpoint.pth\")\n",
    "\n",
    "# # After loading, the model in the trainer instance is now the re-trained model\n",
    "# # and is ready for use in further processing or inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n",
    "dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [01:21<00:00, 81.66s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 53/53 [40:38<00:00, 46.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n",
    "extractor_P = DeepFeatures(trainer.model)\n",
    "query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosine': array([[ 0.09627107,  0.79593915,  0.14943193, ...,  0.69737315,\n",
      "         0.7350316 , -0.05668891],\n",
      "       [ 0.03905028,  0.7565675 ,  0.13760255, ...,  0.73565376,\n",
      "         0.81071985, -0.07374125],\n",
      "       [ 0.02577067,  0.7349705 ,  0.22080323, ...,  0.7731887 ,\n",
      "         0.7957578 , -0.07019792],\n",
      "       ...,\n",
      "       [ 0.09425995,  0.39615124, -0.06429233, ...,  0.37036046,\n",
      "         0.43662593, -0.13101849],\n",
      "       [-0.07259335,  0.5382223 ,  0.10452529, ...,  0.5695142 ,\n",
      "         0.57768214,  0.00940057],\n",
      "       [ 0.01475872, -0.03862431,  0.7133652 , ...,  0.10719028,\n",
      "        -0.01224127, -0.16642839]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "similarity_function = CosineSimilarity()\n",
    "similarity_P = similarity_function(query_P, database_P)\n",
    "print(similarity_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 100 test Images:-\n",
      " ['10_hexing' '37_xinger' '01_aoliao' '33_wuyi' '47_yingying' '15_maosun'\n",
      " '09_fushun' '10_hexing' '36_xingda' '19_nannan' '08_fulai' '31_shurong'\n",
      " '44_yayi' '39_xinghui' '19_nannan' '14_maodou' '14_maodou' '00_aibang'\n",
      " '21_nina' '35_xilan' '19_nannan' '33_wuyi' '47_yingying' '49_yuanrun'\n",
      " '38_xingfan' '04_chengdui' '16_maotao' '48_yongbang' '11_jiaoao'\n",
      " '49_yuanrun' '24_qixi' '10_hexing' '35_xilan' '35_xilan' '02_baolan'\n",
      " '30_shuqing' '31_shurong' '02_baolan' '23_qiubang' '26_qiyuan'\n",
      " '04_chengdui' '35_xilan' '08_fulai' '00_aibang' '28_sa' '29_shuangxiong'\n",
      " '16_maotao' '44_yayi' '14_maodou' '30_shuqing' '36_xingda' '39_xinghui'\n",
      " '33_wuyi' '10_hexing' '20_nike' '00_aibang' '49_yuanrun' '35_xilan'\n",
      " '04_chengdui' '44_yayi' '39_xinghui' '44_yayi' '10_hexing' '36_xingda'\n",
      " '16_maotao' '22_nini' '00_aibang' '39_xinghui' '10_hexing' '17_meibang'\n",
      " '00_aibang' '46_yazhu' '14_maodou' '15_maosun' '32_susu' '26_qiyuan'\n",
      " '27_rourou' '23_qiubang' '18_miaomiao' '48_yongbang' '46_yazhu'\n",
      " '16_maotao' '33_wuyi' '13_jingliang' '41_xingxiao' '11_jiaoao'\n",
      " '09_fushun' '10_hexing' '22_nini' '36_xingda' '09_fushun' '28_sa'\n",
      " '48_yongbang' '47_yingying' '28_sa' '15_maosun' '39_xinghui' '21_nina'\n",
      " '43_yaxing' '47_yingying']\n",
      "Accuracy on IPanda50 data: 82.00%\n",
      "Precision: 0.8880952380952379\n",
      "Recall: 0.82\n",
      "F1 Score: 0.8215151515151515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_tools\\inference\\classifier.py:61: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    }
   ],
   "source": [
    "classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n",
    "predictions_P = classifier_P(similarity_P['cosine'])\n",
    "print(\"Predictions for 100 test Images:-\\n\",predictions_P)\n",
    "accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n",
    "print(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\n",
    "precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "print(\"Precision:\", precision_P)\n",
    "print(\"Recall:\", recall_P)\n",
    "print(\"F1 Score:\", f1_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIPLET_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET IPanda50: DOWNLOADING STARTED.\n",
      "You are trying to download an already downloaded dataset.\n",
      "        This message may have happened to due interrupted download or extract.\n",
      "        To force the download use the `force=True` keyword such as\n",
      "        get_data(..., force=True) or download(..., force=True).\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████| 54/54 [35:11<00:00, 39.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 0.2429884970188141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████| 54/54 [35:36<00:00, 39.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Loss = 0.22894325852394104\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import timm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import SGD\n",
    "\n",
    "from wildlife_tools.data import WildlifeDataset, SplitMetadata\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "\n",
    "import timm\n",
    "import numpy as np\n",
    "from wildlife_datasets.datasets import MacaqueFaces\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, splits\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import timm\n",
    "import itertools\n",
    "from torch.optim import SGD\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer , TripletLoss\n",
    "\n",
    "# Download dataset (if not already downloaded)\n",
    "datasets.IPanda50.get_data('../data/IPanda50')\n",
    "# Load dataset metadata\n",
    "metadata = datasets.IPanda50('../data/IPanda50')\n",
    "transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n",
    "\n",
    "\n",
    "# Download MegaDescriptor-T backbone from HuggingFace Hub\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n",
    "\n",
    "# Arcface loss - needs backbone output size and number of classes.\n",
    "objective = TripletLoss()\n",
    "\n",
    "# Optimize parameters in backbone and in objective using single optimizer.\n",
    "params = itertools.chain(backbone.parameters(), objective.parameters())\n",
    "optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n",
    "\n",
    "def print_epoch_loss(trainer, epoch_data):\n",
    "    # This function will print the average loss at the end of each epoch\n",
    "    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n",
    "\n",
    "\n",
    "trainer = BasicTrainer(\n",
    "    dataset=dataset,\n",
    "    model=backbone,\n",
    "    objective=objective,\n",
    "    optimizer=optimizer,\n",
    "    epochs=2,\n",
    "    device='cuda',\n",
    "    epoch_callback=print_epoch_loss\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainer.save(\"retrained_chks\", file_name=\"tripletloss_IPanda50_retrained_checkpoint.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [02:11<00:00, 131.40s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 53/53 [15:33<00:00, 17.62s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n",
    "dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)\n",
    "\n",
    "# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n",
    "extractor_P = DeepFeatures(trainer.model , device = 'cuda')\n",
    "\n",
    "query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosine': array([[ 0.03701708,  0.521732  ,  0.0868057 , ...,  0.3185835 ,\n",
      "         0.42524353,  0.04476943],\n",
      "       [ 0.11612925,  0.41291755, -0.02290463, ...,  0.32192972,\n",
      "         0.5486889 , -0.1002147 ],\n",
      "       [-0.01923139,  0.22683984,  0.15135822, ...,  0.26983368,\n",
      "         0.32924   , -0.11969242],\n",
      "       ...,\n",
      "       [ 0.18609129,  0.08397243, -0.15004626, ...,  0.04140432,\n",
      "         0.21245056, -0.14800066],\n",
      "       [-0.08609886,  0.07024826,  0.02052286, ...,  0.03947326,\n",
      "         0.12324461, -0.01117898],\n",
      "       [ 0.00100294,  0.16951269,  0.71958387, ...,  0.3686461 ,\n",
      "         0.1663388 , -0.00166226]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "similarity_function = CosineSimilarity()\n",
    "similarity_P = similarity_function(query_P, database_P)\n",
    "print(similarity_P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 100 test Images:-\n",
      " ['10_hexing' '37_xinger' '01_aoliao' '33_wuyi' '47_yingying' '15_maosun'\n",
      " '09_fushun' '16_maotao' '36_xingda' '19_nannan' '08_fulai' '31_shurong'\n",
      " '44_yayi' '25_qiyi' '37_xinger' '14_maodou' '14_maodou' '00_aibang'\n",
      " '21_nina' '35_xilan' '19_nannan' '33_wuyi' '47_yingying' '49_yuanrun'\n",
      " '09_fushun' '34_xiaoqiao' '16_maotao' '48_yongbang' '11_jiaoao'\n",
      " '49_yuanrun' '24_qixi' '10_hexing' '35_xilan' '35_xilan' '02_baolan'\n",
      " '30_shuqing' '31_shurong' '02_baolan' '23_qiubang' '26_qiyuan'\n",
      " '04_chengdui' '35_xilan' '08_fulai' '00_aibang' '28_sa' '29_shuangxiong'\n",
      " '11_jiaoao' '44_yayi' '14_maodou' '30_shuqing' '36_xingda' '39_xinghui'\n",
      " '33_wuyi' '10_hexing' '10_hexing' '00_aibang' '49_yuanrun' '35_xilan'\n",
      " '04_chengdui' '10_hexing' '39_xinghui' '44_yayi' '10_hexing' '36_xingda'\n",
      " '16_maotao' '22_nini' '00_aibang' '39_xinghui' '10_hexing' '17_meibang'\n",
      " '00_aibang' '41_xingxiao' '14_maodou' '15_maosun' '32_susu' '26_qiyuan'\n",
      " '27_rourou' '23_qiubang' '18_miaomiao' '48_yongbang' '45_yayun'\n",
      " '16_maotao' '33_wuyi' '13_jingliang' '37_xinger' '39_xinghui' '09_fushun'\n",
      " '40_xingrong' '22_nini' '28_sa' '09_fushun' '28_sa' '48_yongbang'\n",
      " '47_yingying' '28_sa' '15_maosun' '39_xinghui' '20_nike' '43_yaxing'\n",
      " '47_yingying']\n",
      "Accuracy on IPanda50 data: 85.00%\n",
      "Precision: 0.9119047619047619\n",
      "Recall: 0.85\n",
      "F1 Score: 0.8543809523809525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_tools\\inference\\classifier.py:61: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    }
   ],
   "source": [
    "classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n",
    "predictions_P = classifier_P(similarity_P['cosine'])\n",
    "print(\"Predictions for 100 test Images:-\\n\",predictions_P)\n",
    "accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n",
    "print(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\n",
    "precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "print(\"Precision:\", precision_P)\n",
    "print(\"Recall:\", recall_P)\n",
    "print(\"F1 Score:\", f1_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEW DATASET  DogFaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arcface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import timm\n",
    "import pandas as pd\n",
    "import torchvision.transforms as T\n",
    "from torch.optim import SGD\n",
    "\n",
    "from wildlife_tools.data import WildlifeDataset, SplitMetadata\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "\n",
    "import timm\n",
    "import numpy as np\n",
    "from wildlife_datasets.datasets import MacaqueFaces\n",
    "from wildlife_tools.data import WildlifeDataset\n",
    "import torchvision.transforms as T\n",
    "from wildlife_datasets import datasets, splits\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "from wildlife_tools.inference import KnnClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET DogFaceNet: DOWNLOADING STARTED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DogFaceNet_Dataset_224_1.zip: 75.6MB [00:11, 6.81MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET DogFaceNet: EXTRACTING STARTED.\n",
      "DATASET DogFaceNet: FINISHED.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████████████████████| 66/66 [49:59<00:00, 45.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 39.38665771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████| 66/66 [52:05<00:00, 47.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Loss = 38.245521545410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import itertools\n",
    "from torch.optim import SGD\n",
    "from wildlife_tools.train import ArcFaceLoss, BasicTrainer\n",
    "\n",
    "\n",
    "\n",
    "# Download dataset (if not already downloaded)\n",
    "datasets.DogFaceNet.get_data('../data/DogFaceNet')\n",
    "\n",
    "# Load dataset metadata\n",
    "metadata = datasets.DogFaceNet('../data/DogFaceNet')\n",
    "transform = T.Compose([T.Resize([224, 224]), T.ToTensor(), T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "dataset = WildlifeDataset(metadata.df, metadata.root, transform=transform)\n",
    "\n",
    "\n",
    "# Download MegaDescriptor-T backbone from HuggingFace Hub\n",
    "backbone = timm.create_model('hf-hub:BVRA/MegaDescriptor-T-224', num_classes=0, pretrained=True)\n",
    "\n",
    "# Arcface loss - needs backbone output size and number of classes.\n",
    "objective = ArcFaceLoss(\n",
    "    num_classes=dataset.num_classes,\n",
    "    embedding_size=768,\n",
    "    margin=0.5,\n",
    "    scale=64\n",
    "    )\n",
    "\n",
    "# Optimize parameters in backbone and in objective using single optimizer.\n",
    "params = itertools.chain(backbone.parameters(), objective.parameters())\n",
    "optimizer = SGD(params=params, lr=0.001, momentum=0.9)\n",
    "\n",
    "def print_epoch_loss(trainer, epoch_data):\n",
    "    # This function will print the average loss at the end of each epoch\n",
    "    print(f\"Epoch {trainer.epoch}: Average Loss = {epoch_data['train_loss_epoch_avg']}\")\n",
    "\n",
    "\n",
    "trainer = BasicTrainer(\n",
    "    dataset=dataset,\n",
    "    model=backbone,\n",
    "    objective=objective,\n",
    "    optimizer=optimizer,\n",
    "    epochs=2,\n",
    "    device='cuda',\n",
    "    epoch_callback=print_epoch_loss\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save(\"retrained_chks\", file_name=\"arcfaceloass_DogFaceNet_retrained_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_database_P = WildlifeDataset(metadata.df.iloc[100:,:], metadata.root, transform=transform)\n",
    "dataset_query_P = WildlifeDataset(metadata.df.iloc[:100,:], metadata.root, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [02:37<00:00, 157.55s/it]\n",
      "100%|███████████████████████████████████████████████████████████████| 65/65 [37:53<00:00, 34.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# name = 'hf-hub:BVRA/MegaDescriptor-T-224'\n",
    "extractor_P = DeepFeatures(trainer.model)\n",
    "query_P, database_P = extractor_P(dataset_query_P), extractor_P(dataset_database_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cosine': array([[0.90336466, 0.8971159 , 0.9150417 , ..., 0.8279973 , 0.9098954 ,\n",
      "        0.9029107 ],\n",
      "       [0.8997017 , 0.90142804, 0.90338266, ..., 0.83960414, 0.9001368 ,\n",
      "        0.90807635],\n",
      "       [0.9230675 , 0.84706867, 0.92239344, ..., 0.8194646 , 0.83554107,\n",
      "        0.92282724],\n",
      "       ...,\n",
      "       [0.9422437 , 0.8484241 , 0.9454235 , ..., 0.79742295, 0.8450827 ,\n",
      "        0.9554483 ],\n",
      "       [0.89716667, 0.9259574 , 0.89031935, ..., 0.8672122 , 0.90035975,\n",
      "        0.8920688 ],\n",
      "       [0.8957627 , 0.90073335, 0.88797677, ..., 0.85030484, 0.9095253 ,\n",
      "        0.88683367]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "similarity_function = CosineSimilarity()\n",
    "similarity_P = similarity_function(query_P, database_P)\n",
    "print(similarity_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 100 test Images:-\n",
      " ['329' '182' '435' '1122' '428' '1299' '1063' '981' '568' '405' '764'\n",
      " '190' '1201' '1339' '432' '458' '1075' '274' '141' '234' '1104' '991'\n",
      " '329' '219' '269' '488' '1268' '372' '1023' '491' '1376' '448' '1336'\n",
      " '251' '448' '414' '734' '210' '345' '763' '943' '1324' '158' '127' '277'\n",
      " '1274' '1139' '208' '626' '424' '293' '415' '1408' '835' '373' '192'\n",
      " '137' '116' '1314' '957' '329' '385' '189' '23' '1345' '631' '367' '646'\n",
      " '26' '1368' '449' '722' '174' '75' '1169' '458' '473' '149' '373' '622'\n",
      " '817' '677' '589' '1201' '782' '1357' '361' '462' '1324' '766' '183'\n",
      " '200' '445' '191' '397' '187' '270' '563' '1020' '1074']\n",
      "Accuracy on IPanda50 data: 57.00%\n",
      "Precision: 0.9883333333333333\n",
      "Recall: 0.57\n",
      "F1 Score: 0.5683333333333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA_Jupyter2024\\Lib\\site-packages\\wildlife_tools\\inference\\classifier.py:61: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  results = pd.DataFrame(results).T.fillna(method=\"ffill\").T\n"
     ]
    }
   ],
   "source": [
    "classifier_P = KnnClassifier(k=1, database_labels=dataset_database_P.labels_string)\n",
    "predictions_P = classifier_P(similarity_P['cosine'])\n",
    "print(\"Predictions for 100 test Images:-\\n\",predictions_P)\n",
    "accuracy_P = np.mean(dataset_query_P.labels_string == predictions_P)\n",
    "print(\"Accuracy on IPanda50 data: {:.2f}%\".format(accuracy_P * 100))\n",
    "precision_P = precision_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "recall_P = recall_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "f1_P = f1_score(dataset_query_P.labels_string, predictions_P, average='weighted',zero_division=1)\n",
    "print(\"Precision:\", precision_P)\n",
    "print(\"Recall:\", recall_P)\n",
    "print(\"F1 Score:\", f1_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
